In [1]:
import warnings
warnings.filterwarnings('ignore')
In [2]:
import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter
In [3]:
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import confusion_matrix
from imblearn.metrics import classification_report_imbalanced
Read the CSV and Perform Basic Data Cleaning
In [4]:
# https://help.lendingclub.com/hc/en-us/articles/215488038-What-do-the-different-Note-statuses-mean-

columns = [
    "loan_amnt", "int_rate", "installment", "home_ownership",
    "annual_inc", "verification_status", "issue_d", "loan_status",
    "pymnt_plan", "dti", "delinq_2yrs", "inq_last_6mths",
    "open_acc", "pub_rec", "revol_bal", "total_acc",
    "initial_list_status", "out_prncp", "out_prncp_inv", "total_pymnt",
    "total_pymnt_inv", "total_rec_prncp", "total_rec_int", "total_rec_late_fee",
    "recoveries", "collection_recovery_fee", "last_pymnt_amnt", "next_pymnt_d",
    "collections_12_mths_ex_med", "policy_code", "application_type", "acc_now_delinq",
    "tot_coll_amt", "tot_cur_bal", "open_acc_6m", "open_act_il",
    "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il",
    "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc",
    "all_util", "total_rev_hi_lim", "inq_fi", "total_cu_tl",
    "inq_last_12m", "acc_open_past_24mths", "avg_cur_bal", "bc_open_to_buy",
    "bc_util", "chargeoff_within_12_mths", "delinq_amnt", "mo_sin_old_il_acct",
    "mo_sin_old_rev_tl_op", "mo_sin_rcnt_rev_tl_op", "mo_sin_rcnt_tl", "mort_acc",
    "mths_since_recent_bc", "mths_since_recent_inq", "num_accts_ever_120_pd", "num_actv_bc_tl",
    "num_actv_rev_tl", "num_bc_sats", "num_bc_tl", "num_il_tl",
    "num_op_rev_tl", "num_rev_accts", "num_rev_tl_bal_gt_0",
    "num_sats", "num_tl_120dpd_2m", "num_tl_30dpd", "num_tl_90g_dpd_24m",
    "num_tl_op_past_12m", "pct_tl_nvr_dlq", "percent_bc_gt_75", "pub_rec_bankruptcies",
    "tax_liens", "tot_hi_cred_lim", "total_bal_ex_mort", "total_bc_limit",
    "total_il_high_credit_limit", "hardship_flag", "debt_settlement_flag"
]

target = ["loan_status"]
In [5]:
# Load the data
file_path = Path('resources/LoanStats_2019Q1.csv')
df = pd.read_csv(file_path, skiprows=1)[:-2]
df = df.loc[:, columns].copy()

# Drop the null columns where all values are null
df = df.dropna(axis='columns', how='all')

# Drop the null rows
df = df.dropna()

# Remove the `Issued` loan status
issued_mask = df['loan_status'] != 'Issued'
df = df.loc[issued_mask]

# convert interest rate to numerical
df['int_rate'] = df['int_rate'].str.replace('%', '')
df['int_rate'] = df['int_rate'].astype('float') / 100


# Convert the target column values to low_risk and high_risk based on their values
x = {'Current': 'low_risk'}   
df = df.replace(x)

x = dict.fromkeys(['Late (31-120 days)', 'Late (16-30 days)', 'Default', 'In Grace Period'], 'high_risk')    
df = df.replace(x)

df.reset_index(inplace=True, drop=True)

df.head()
Out[5]:
loan_amnt	int_rate	installment	home_ownership	annual_inc	verification_status	issue_d	loan_status	pymnt_plan	dti	...	pct_tl_nvr_dlq	percent_bc_gt_75	pub_rec_bankruptcies	tax_liens	tot_hi_cred_lim	total_bal_ex_mort	total_bc_limit	total_il_high_credit_limit	hardship_flag	debt_settlement_flag
0	10500.0	0.1719	375.35	RENT	66000.0	Source Verified	Mar-2019	low_risk	n	27.24	...	85.7	100.0	0.0	0.0	65687.0	38199.0	2000.0	61987.0	N	N
1	25000.0	0.2000	929.09	MORTGAGE	105000.0	Verified	Mar-2019	low_risk	n	20.23	...	91.2	50.0	1.0	0.0	271427.0	60641.0	41200.0	49197.0	N	N
2	20000.0	0.2000	529.88	MORTGAGE	56000.0	Verified	Mar-2019	low_risk	n	24.26	...	66.7	50.0	0.0	0.0	60644.0	45684.0	7500.0	43144.0	N	N
3	10000.0	0.1640	353.55	RENT	92000.0	Verified	Mar-2019	low_risk	n	31.44	...	100.0	50.0	1.0	0.0	99506.0	68784.0	19700.0	76506.0	N	N
4	22000.0	0.1474	520.39	MORTGAGE	52000.0	Not Verified	Mar-2019	low_risk	n	18.76	...	100.0	0.0	0.0	0.0	219750.0	25919.0	27600.0	20000.0	N	N
5 rows × 86 columns

Split the Data into Training and Testing
In [6]:
# Create our features
X = pd.get_dummies(df, columns=["home_ownership","verification_status", "issue_d", "pymnt_plan","initial_list_status","next_pymnt_d", "application_type","hardship_flag", "debt_settlement_flag"])
X = X.drop(["loan_status"], axis=1)

# Create our target
y = df["loan_status"]
In [7]:
X.describe()
Out[7]:
loan_amnt	int_rate	installment	annual_inc	dti	delinq_2yrs	inq_last_6mths	open_acc	pub_rec	revol_bal	...	issue_d_Mar-2019	pymnt_plan_n	initial_list_status_f	initial_list_status_w	next_pymnt_d_Apr-2019	next_pymnt_d_May-2019	application_type_Individual	application_type_Joint App	hardship_flag_N	debt_settlement_flag_N
count	68817.000000	68817.000000	68817.000000	6.881700e+04	68817.000000	68817.000000	68817.000000	68817.000000	68817.000000	68817.000000	...	68817.000000	68817.0	68817.000000	68817.000000	68817.000000	68817.000000	68817.000000	68817.000000	68817.0	68817.0
mean	16677.594562	0.127718	480.652863	8.821371e+04	21.778153	0.217766	0.497697	12.587340	0.126030	17604.142828	...	0.177238	1.0	0.123879	0.876121	0.383161	0.616839	0.860340	0.139660	1.0	1.0
std	10277.348590	0.048130	288.062432	1.155800e+05	20.199244	0.718367	0.758122	6.022869	0.336797	21835.880400	...	0.381873	0.0	0.329446	0.329446	0.486161	0.486161	0.346637	0.346637	0.0	0.0
min	1000.000000	0.060000	30.890000	4.000000e+01	0.000000	0.000000	0.000000	2.000000	0.000000	0.000000	...	0.000000	1.0	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.0	1.0
25%	9000.000000	0.088100	265.730000	5.000000e+04	13.890000	0.000000	0.000000	8.000000	0.000000	6293.000000	...	0.000000	1.0	0.000000	1.000000	0.000000	0.000000	1.000000	0.000000	1.0	1.0
50%	15000.000000	0.118000	404.560000	7.300000e+04	19.760000	0.000000	0.000000	11.000000	0.000000	12068.000000	...	0.000000	1.0	0.000000	1.000000	0.000000	1.000000	1.000000	0.000000	1.0	1.0
75%	24000.000000	0.155700	648.100000	1.040000e+05	26.660000	0.000000	1.000000	16.000000	0.000000	21735.000000	...	0.000000	1.0	0.000000	1.000000	1.000000	1.000000	1.000000	0.000000	1.0	1.0
max	40000.000000	0.308400	1676.230000	8.797500e+06	999.000000	18.000000	5.000000	72.000000	4.000000	587191.000000	...	1.000000	1.0	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.0	1.0
8 rows × 95 columns

In [8]:
# Check the balance of our target values
y.value_counts()
Out[8]:
low_risk     68470
high_risk      347
Name: loan_status, dtype: int64
In [9]:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=28)
Counter(y_train)
Out[9]:
Counter({'low_risk': 51339, 'high_risk': 273})
Ensemble Learners
In this section, you will compare two ensemble algorithms to determine which algorithm results in the best performance. You will train a Balanced Random Forest Classifier and an Easy Ensemble AdaBoost classifier . For each algorithm, be sure to complete the folliowing steps:

Train the model using the training data.
Calculate the balanced accuracy score from sklearn.metrics.
Print the confusion matrix from sklearn.metrics.
Generate a classication report using the imbalanced_classification_report from imbalanced-learn.
For the Balanced Random Forest Classifier onely, print the feature importance sorted in descending order (most important feature to least important) along with the feature score
Note: Use a random state of 1 for each algorithm to ensure consistency between tests

Balanced Random Forest Classifier
In [10]:
# Resample the training data with the BalancedRandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=28)
rf_model = rf_model.fit(X_train, y_train)
In [11]:
# Calculated the balanced accuracy score
from sklearn.metrics import balanced_accuracy_score

y_pred = rf_model.predict(X_test)
acc_score = balanced_accuracy_score(y_test, y_pred)
In [12]:
# Display the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Create df for confusion matrix
cm_df = pd.DataFrame(
    cm, index=["Actual High Risk", "Actual Low Risk"], columns=["Predicted High Risk", "Predicted Low Risk"])

cm_df
Out[12]:
Predicted High Risk	Predicted Low Risk
Actual High Risk	22	52
Actual Low Risk	17	17114
In [13]:
# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced

# Displaying results
print("Random Forest Analysis")
print("Confusion Matrix")
display(cm_df)
print(f"Accuracy Score : {acc_score}")
print("Classification Report")
print(classification_report_imbalanced(y_test, y_pred))
Random Forest Analysis
Confusion Matrix
Predicted High Risk	Predicted Low Risk
Actual High Risk	22	52
Actual Low Risk	17	17114
Accuracy Score : 0.6481524721265542
Classification Report
                   pre       rec       spe        f1       geo       iba       sup

  high_risk       0.56      0.30      1.00      0.39      0.54      0.28        74
   low_risk       1.00      1.00      0.30      1.00      0.54      0.32     17131

avg / total       1.00      1.00      0.30      1.00      0.54      0.32     17205

In [14]:
# List the features sorted in descending order by feature importance
importances = rf_model.feature_importances_
sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)
Out[14]:
[(0.0925950953591137, 'total_rec_prncp'),
 (0.07839610498940712, 'last_pymnt_amnt'),
 (0.07584190582399367, 'total_pymnt'),
 (0.06883185845882185, 'total_rec_int'),
 (0.061308990371240615, 'total_pymnt_inv'),
 (0.022813986199687934, 'out_prncp_inv'),
 (0.02109392161411433, 'out_prncp'),
 (0.020401333802407263, 'installment'),
 (0.017005309784297828, 'mo_sin_old_rev_tl_op'),
 (0.015499270089711106, 'dti'),
 (0.014970114650454858, 'mo_sin_old_il_acct'),
 (0.01470461386311724, 'tot_cur_bal'),
 (0.014540169445729036, 'loan_amnt'),
 (0.013865104164327928, 'total_bal_ex_mort'),
 (0.013788687635669502, 'revol_bal'),
 (0.013661454866103399, 'total_bal_il'),
 (0.013652080109922249, 'max_bal_bc'),
 (0.013460501403283573, 'total_rev_hi_lim'),
 (0.013167417240710558, 'avg_cur_bal'),
 (0.012973238444462213, 'tot_hi_cred_lim'),
 (0.012738457312220322, 'total_il_high_credit_limit'),
 (0.011984501911907728, 'annual_inc'),
 (0.011460986011149513, 'bc_open_to_buy'),
 (0.011303588577648724, 'bc_util'),
 (0.011217402555543814, 'total_acc'),
 (0.01111420889709516, 'all_util'),
 (0.010947451704231745, 'mths_since_recent_bc'),
 (0.010929952601523319, 'int_rate'),
 (0.010504012425337704, 'mths_since_rcnt_il'),
 (0.010396984507574114, 'num_il_tl'),
 (0.010012134132579993, 'mths_since_recent_inq'),
 (0.009939346833987627, 'il_util'),
 (0.00972424072037061, 'total_bc_limit'),
 (0.009305235944327923, 'num_sats'),
 (0.008586535454315846, 'issue_d_Mar-2019'),
 (0.00857488612624351, 'acc_open_past_24mths'),
 (0.008455672111740968, 'open_acc'),
 (0.008426308839501188, 'inq_last_12m'),
 (0.00817362472641478, 'mo_sin_rcnt_tl'),
 (0.008107474973508099, 'mo_sin_rcnt_rev_tl_op'),
 (0.008016307125678022, 'num_actv_bc_tl'),
 (0.007882224920021478, 'num_actv_rev_tl'),
 (0.007850193626975562, 'num_rev_tl_bal_gt_0'),
 (0.007820970168417466, 'num_rev_accts'),
 (0.007531260377288582, 'total_rec_late_fee'),
 (0.007496998773346427, 'num_op_rev_tl'),
 (0.007210452658476718, 'num_bc_tl'),
 (0.006985104332400469, 'open_rv_24m'),
 (0.0069141415454868115, 'inq_fi'),
 (0.006766762532175841, 'total_cu_tl'),
 (0.006377549663802725, 'open_il_24m'),
 (0.006233793169720202, 'pct_tl_nvr_dlq'),
 (0.006202360712041257, 'mort_acc'),
 (0.006154602029619332, 'num_tl_op_past_12m'),
 (0.006032499443534684, 'open_acc_6m'),
 (0.0058267153215110735, 'num_bc_sats'),
 (0.0056905204456215285, 'open_act_il'),
 (0.005622367259351426, 'percent_bc_gt_75'),
 (0.00532454171782641, 'inq_last_6mths'),
 (0.004728753058087165, 'open_rv_12m'),
 (0.004291978317330483, 'tot_coll_amt'),
 (0.004070236123254959, 'open_il_12m'),
 (0.003918495155010892, 'issue_d_Jan-2019'),
 (0.0038433929612692627, 'next_pymnt_d_May-2019'),
 (0.0037050231654757087, 'num_accts_ever_120_pd'),
 (0.0034385246560002226, 'next_pymnt_d_Apr-2019'),
 (0.0033448395828513276, 'issue_d_Feb-2019'),
 (0.0030669628800227325, 'delinq_2yrs'),
 (0.002461028951171136, 'verification_status_Verified'),
 (0.002122958028228153, 'verification_status_Not Verified'),
 (0.0020011902719086067, 'pub_rec_bankruptcies'),
 (0.0019609497849113603, 'verification_status_Source Verified'),
 (0.0018116439106557831, 'application_type_Joint App'),
 (0.0017437658458188299, 'pub_rec'),
 (0.001543242314450663, 'home_ownership_MORTGAGE'),
 (0.0014680383654483236, 'home_ownership_OWN'),
 (0.0014557859006519799, 'num_tl_90g_dpd_24m'),
 (0.0013017932425787853, 'application_type_Individual'),
 (0.0012695580643123248, 'initial_list_status_f'),
 (0.0011621414795986332, 'initial_list_status_w'),
 (0.0010947595566796976, 'collections_12_mths_ex_med'),
 (0.0010852906755085712, 'home_ownership_RENT'),
 (0.00040559981399645496, 'home_ownership_ANY'),
 (0.0002905173876833233, 'chargeoff_within_12_mths'),
 (0.0, 'tax_liens'),
 (0.0, 'recoveries'),
 (0.0, 'pymnt_plan_n'),
 (0.0, 'policy_code'),
 (0.0, 'num_tl_30dpd'),
 (0.0, 'num_tl_120dpd_2m'),
 (0.0, 'hardship_flag_N'),
 (0.0, 'delinq_amnt'),
 (0.0, 'debt_settlement_flag_N'),
 (0.0, 'collection_recovery_fee'),
 (0.0, 'acc_now_delinq')]
Easy Ensemble AdaBoost Classifier
In [15]:
# Train the EasyEnsembleClassifier
from imblearn.ensemble import EasyEnsembleClassifier
ee_model = EasyEnsembleClassifier(n_estimators=100, random_state=28)
ee_model = ee_model.fit(X_train, y_train)
In [16]:
# Calculated the balanced accuracy score
y_pred = ee_model.predict(X_test)
acc_score = balanced_accuracy_score(y_test, y_pred)
In [17]:
# Display the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Create df for confusion matrix
cm_df = pd.DataFrame(
    cm, index=["Actual High Risk", "Actual Low Risk"], columns=["Predicted High Risk", "Predicted Low Risk"])

cm_df
Out[17]:
Predicted High Risk	Predicted Low Risk
Actual High Risk	67	7
Actual Low Risk	1018	16113
In [18]:
# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced

# Displaying results
print("Easy Ensemble Analysis")
print("Confusion Matrix")
display(cm_df)
print(f"Accuracy Score : {acc_score}")
print("Classification Report")
print(classification_report_imbalanced(y_test, y_pred))
Easy Ensemble Analysis
Confusion Matrix
Predicted High Risk	Predicted Low Risk
Actual High Risk	67	7
Actual Low Risk	1018	16113
Accuracy Score : 0.9229904850855175
Classification Report
                   pre       rec       spe        f1       geo       iba       sup

  high_risk       0.06      0.91      0.94      0.12      0.92      0.85        74
   low_risk       1.00      0.94      0.91      0.97      0.92      0.85     17131

avg / total       1.00      0.94      0.91      0.97      0.92      0.85     17205

In [ ]:
